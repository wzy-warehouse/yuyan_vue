<template>
  <div class="page-container">
    <!-- 文档模板组件：统一渲染标题、副标题、元数据和正文 -->
    <DocumentTemplate
      :title="currentDoc.title"
      :subtitle="currentDoc.subtitle"
      :meta="currentDoc.meta"
      :content="currentDoc.htmlContent"
      :title-align="'center'"
    />
  </div>
</template>

<script setup>
import { ref, computed } from 'vue';
import { useRoute } from 'vue-router'; // 用于获取路由参数
import DocumentTemplate from "@/components/template.vue";

// 定义5篇文章的完整数据
const docList = ref([
  // 第1篇：远程无人机洪灾应急信息全自主感知技术
  {
    title: "远程无人机洪灾应急信息全自主感知技术",
    subtitle: "武汉大学测绘遥感信息工程国家重点实验室研究成果",
    meta: "李德仁、眭海刚、刘俊怡 | 遥感技术前沿",
    htmlContent: `
      <h2>1 前言</h2>
      <p>洪涝灾害是一种常见的自然灾害，对人类生命安全、基础设施、房屋建筑、农业、自然环境和经济有着的广泛影响。洪灾的爆发具有突发性和破坏性，洪灾发生后减灾救灾工作迫切需要第一时间获取全局的洪涝灾害应急信息，包括洪水淹没范围、洪水持续时间、典型承灾体受灾情况、受灾人员/车辆位置信息等。无人机遥感技术具有数据采集快、远距离观测、实时性好、动态性强等不可替代的优势，特别是重大自然灾害“三断”（断电断网断路）等极端情况下，已成为支撑灾害应急监测、预警与评估的重要手段。</p>
      <h2>2 研究现状与技术挑战</h2>
      <p>由于洪灾应急的时效性需求，航空遥感特别是无人机遥感灾害信息实时处理一直是国内外学术界和产业界关注的焦点之一。城市区域的洪灾应急往往需要航空遥感提供更加快捷智能的灾情信息服务，包括洪水淹没范围、受困救援目标、重要设施受损情况等。将深度学习算法与航空遥感图像/视频相结合，不仅可以有效提高洪灾信息提取的准确性和时效性，而且还能从航空视频数据中实时识别和定位受灾人群，为应急救援提供有力的救援指引信息。</p>
      <p>尽管航空遥感在洪涝灾害监测与应用中发挥了重要作用，但在实时性、精准性等方面仍面临如下挑战： </p>
      <ul style="list-style-type: none;">
        <li>（1）洪涝应急响应不实时，航空遥感面临“操作起飞-获取数据-应急处理”等过程，应急响应时间一般是小时级，机载实时处理与传输未大规模应用，实时性仍是目前遥感急响应的难言之隐。</li>
        <li>（2）洪涝灾害要素提取不全面。绝大多数遥感灾害信息提取主要关注洪水淹没的范围、道路/建筑物等典型承载体受灾情况等，缺少对受灾车辆、受灾人员的动态定位与跟踪；同时灾后遥感信息处理的自动化和精准化水平还有待进一步提高。</li>
      </ul>

      <h2>3 远程无人机洪灾应急信息全自主感知</h2>
      <p>远程无人机洪灾应急信息全自主感知系统是利用无人机集成北斗和遥感观测手段，当地面传感网由于断电或损毁无法正常工作，而交通瘫痪（如水淹路断）又使得地面人员和车辆难以到达灾害区域时，通过部署在灾害风险区域的远程无人机全自动快速感知系统，可实现在三断（断电、断水、断路）等极端条件下对的灾害现场远程全自动快速感知。该系统具有：“远程一键收发、自动规划飞行、自动快速成图、自动损毁提取、信息自动回传、克服三断环境”的特点。在断电的极端条件下，通过备用电池和太阳能板来为机巢和设备供电；在断网的情况下，基于北斗短报文或卫星通信手段，远程一键启动系统并自主规划任务快速获取影像，提取受灾范围和损毁信息，通过北斗短报文或长距离无线自组网直接传输至指挥中心，生成灾损图，实现对灾害区域的快速感知，在极端条件下能够在分钟级可完成从发生灾害到获取灾害第一张图的应急响应。</p>
      <div class="image-container">
        <img src="/src/assets/images/technology/drone-flood-1.png" alt="远程无人机全自动快速感知流程图" />
        <div class="image-caption">图3 面向应急的远程无人机全自动快速感知流程图</div>
      </div>
      <p>受灾区域的洪灾场景复杂和无人机影像地物覆盖类型多样，导致现有深度语义分割模型的无人机影像洪水范围提取精度较差、受灾目标实时检测算法相对较少。针对面临边缘端计算资源受限、模型推理速度不足、受困目标识别不准的问题，我们提出了基于无人机远程感知的局部洪灾应急信息精细提取方法，并将该方法部署在远程无人机感知系统中，实现了洪水范围精细提取与洪灾受困人、车等典型目标的实时检测。</p>
      <div class="image-container">
        <img src="/src/assets/images/technology/drone-flood-2.png" alt="广西浦北部署点与监测结果" style="width:48%;margin-right:2%;height: 400px" />
        <img src="/src/assets/images/technology/drone-flood-3.png" alt="无人机影像受灾目标监测结果" style="width:48%;height: 400px" />
        <div class="image-caption">图4 广西壮族自治区钦州市浦北县远程无人机感知系统：(a)部署点 (b)受灾目标监测结果</div>
      </div>

      <h2>4 发展趋势展望</h2>
      <p>航空遥感已成为洪涝灾害监测的常态化手段，联合多模态遥感影像进行洪灾应急信息提取工作，在洪涝灾前预警分析、灾中应急自动制图、灾后损失评估的智能化遥感空间信息服务能力还有很大提升空间。未来将围绕以下方向发展：</p>
      <ul style="list-style-type: none;">
        <li>（1）协同多平台无人系统：利用无人机集群、无人船、水下无人潜行器、地面无人值守观测平台等多源遥感协同监测手段，增强对山洪、水利设施安全隐患、水下管涌渗漏点等传统遥感关注不多的监测能力。</li>
        <li>（2）加快遥感与人工智能的紧耦合：利用人工智能大模型（遥感大模型、视觉大模型、语言大模型）实现多样化遥感数据的全自动提取，替代目前依赖人工的灾损信息提取和隐患点分析方式。</li>
      </ul>
    `
  },
  // 第2篇：基于道路中心线的分阶段弱监督遥感图像道路提取
  {
    title: "基于道路中心线的分阶段弱监督遥感图像道路提取",
    subtitle: "《北邮学报》刊文 | 降低像素级标注成本的创新方案",
    meta: "王薇、谢沈惟、闫浩田等 | 应急管理部国家减灾中心/北京邮电大学",
    htmlContent: `
      <h2>1 弱监督道路提取概况</h2>
      <p>道路状况监测对城市发展和基础设施建设至关重要。随着遥感技术的发展，语义分割算法成为从卫星图像中提取道路的主流方法。然而，由于不同地理环境下道路纹理复杂多变，且像素级标注成本高昂，获取大量高分辨率卫星图像的像素级标注用于训练非常困难。</p>
      <p>弱监督学习方法通过利用易获取的道路中心线矢量数据，减少了对全像素标注的需求。现有基于道路中心线的弱监督方法通常通过非深度学习方式生成伪掩码，导致道路边缘信息缺失。本文提出的“分阶段弱监督道路提取算法（SWSS）”旨在解决这些问题，提高道路分割的准确性和完整性。</p>
      <div class="image-container">
        <img src="/images/road-extraction-1.png" alt="现有伪道路标注掩码生成方式" />
        <div class="image-caption">图1 现有文献中的伪道路标注掩码生成方式：(a)Facebook团队栅格化操作 (b)WeaklyOSM方法 (c)ScRoadExtractor方法</div>
      </div>

      <h2>2 SWSS算法方案</h2>
      <p>SWSS算法通过两个核心阶段实现道路提取，并引入创新策略优化模型性能：</p>
      <h3>2.1 双阶段训练流程</h3>
      <ul>
        <li><strong>第一阶段：中心线学习</strong>：利用道路中心线涂鸦标签进行训练，学习道路的大致位置和拓扑结构信息，为后续精细分割奠定基础。</li>
        <li><strong>第二阶段：伪掩码学习</strong>：使用第一阶段生成的伪掩码进行训练，逐步优化道路分割结果，提升边缘精度和道路完整性。</li>
      </ul>
      <h3>2.2 关键优化策略</h3>
      <ul>
        <li><strong>伪掩码更新策略</strong>：通过指数移动平均方法周期性地更新伪掩码，持续提升伪标签质量，减少初始掩码误差对模型的影响。</li>
        <li><strong>混合训练策略</strong>：固定部分真实道路分割掩码，引导模型聚焦道路特征，减少对非道路纹理的错误关注。</li>
        <li><strong>定制化损失函数</strong>：设计适用于道路前景和背景的损失函数，进一步优化模型训练，平衡正负样本学习权重。</li>
      </ul>
      <div class="image-container">
        <img src="/images/road-extraction-2.png" alt="SWSS算法流程图" />
        <div class="image-caption">图2 SWSS算法流程图</div>
      </div>

      <h2>3 仿真实验</h2>
      <p>实验在DeepGlobe（全球道路数据集）和云南山区道路数据集上进行，以交并比（IoU）作为核心评估指标，验证算法性能。</p>
      <h3>3.1 实验结果对比</h3>
      <table border="1" cellpadding="8" cellspacing="0" style="width:100%;border-collapse:collapse;">
        <caption>表1 基于DeepGlobe数据的结果对比</caption>
        <thead>
          <tr style="background:#f5f5f5;">
            <<th>监督方式</</th>
            <<th>方法</</th>
            <<th>IoU</</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="4">弱监督</td>
            <td>Facebook团队</td>
            <td>0.5241</td>
          </tr>
          <tr>
            <td>WeaklyOSM</td>
            <td>0.5239</td>
          </tr>
          <tr>
            <td>ScRoadExtractor</td>
            <td>0.5782</td>
          </tr>
          <tr>
            <td>SWSSRoadSegmentor</td>
            <td>0.5621</td>
          </tr>
          <tr>
            <td>弱监督（优化后）</td>
            <td>SWSSRoadSegmentor + Mix + Denoising</td>
            <td>0.5801</td>
          </tr>
          <tr>
            <td>全监督</td>
            <td>DLinkNet</td>
            <td>0.6317</td>
          </tr>
        </tbody>
      </table>

      <table border="1" cellpadding="8" cellspacing="0" style="width:100%;border-collapse:collapse;margin-top:16px;">
        <caption>表2 基于云南山区数据的结果对比</caption>
        <thead>
          <tr style="background:#f5f5f5;">
            <<th>方法</</th>
            <<th>IoU</</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Facebook团队</td>
            <td>0.3270</td>
          </tr>
          <tr>
            <td>SWSSRoadSegmentor</td>
            <td>0.5077</td>
          </tr>
          <tr>
            <td>DLinkNet（全监督）</td>
            <td>0.4223</td>
          </tr>
        </tbody>
      </table>

      <h3>3.2 消融实验验证</h3>
      <table border="1" cellpadding="8" cellspacing="0" style="width:100%;border-collapse:collapse;">
        <caption>表3 训练策略消融实验（DeepGlobe数据集）</caption>
        <thead>
          <tr style="background:#f5f5f5;">
            <<th>策略</</th>
            <<th>IoU</</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>SWSSRoadSegmentor（基础版）</td>
            <td>0.5621</td>
          </tr>
          <tr>
            <td>SWSSRoadSegmentor + 混合训练（Mix）</td>
            <td>0.5787</td>
          </tr>
          <tr>
            <td>SWSSRoadSegmentor + 去噪模块（Denoising）</td>
            <td>0.5681</td>
          </tr>
          <tr>
            <td>SWSSRoadSegmentor + Mix + Denoising</td>
            <td>0.5801</td>
          </tr>
        </tbody>
      </table>

      <table border="1" cellpadding="8" cellspacing="0" style="width:100%;border-collapse:collapse;margin-top:16px;">
        <caption>表4 损失函数权重消融实验</caption>
        <thead>
          <tr style="background:#f5f5f5;">
            <<th>道路前景损失权重</</th>
            <<th>道路背景损失权重</</th>
            <<th>IoU</</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1.0</td>
            <td>0.1</td>
            <td>0.5534</td>
          </tr>
          <tr>
            <td>1.0</td>
            <td>0.5</td>
            <td>0.5621</td>
          </tr>
          <tr>
            <td>1.0</td>
            <td>1.0</td>
            <td>0.5602</td>
          </tr>
          <tr>
            <td>0.5</td>
            <td>0.5</td>
            <td>0.5561</td>
          </tr>
          <tr>
            <td>0.5</td>
            <td>2.0</td>
            <td>0.5610</td>
          </tr>
        </tbody>
      </table>
      <p><strong>关键结论</strong>：SWSS算法在山区复杂场景（云南数据集IoU 0.5077）表现优于全监督方法DLinkNet，且通过混合训练+去噪模块优化后，DeepGlobe数据集IoU提升至0.5801，显著优于其他弱监督方法。</p>

      <h2>4 结果对比与结论</h2>
      <div class="image-container">
        <img src="/images/road-extraction-3.png" alt="SWSS与主流方法道路提取结果对比" />
        <div class="image-caption">图4 SWSS与主流方法的道路提取结果对比：(a)原始卫星图像 (b)道路中心线弱标签 (c)Facebook团队方法 (d)WeaklyOSM方法 (e)ScRoadExtractor方法 (f)SWSS方法 (g)真实道路分割标签</div>
      </div>
      <p>对比结果显示：</p>
      <ul>
        <li>Facebook团队方法：道路完整性和连通性存在明显劣势，易出现断裂。</li>
        <li>WeaklyOSM方法：道路边缘特征提取不足，边界模糊。</li>
        <li>ScRoadExtractor方法：道路拓扑结构与真实标签差距较大，易误判非道路区域。</li>
        <li>SWSS方法：预测的道路掩码更全面，纹理更精确，边缘轮廓和拓扑结构与真实标签一致性最高。</li>
      </ul>
      <p><strong>结论</strong>：本文提出的基于道路中心线的分阶段弱监督道路提取算法，通过伪掩码更新策略和混合训练策略，有效提高了道路分割的准确性和完整性，为降低卫星图像道路提取任务中的数据标注成本提供了新的解决方案。</p>
    `
  },
  // 第3篇：高分辨率遥感影像复杂目标检测技术及减灾应用
  {
    title: "高分辨率遥感影像复杂目标检测技术及减灾应用",
    subtitle: "北航团队研发 | 支撑90余次重大灾害灾后评估",
    meta: "刘庆杰、张明明、万峤 | 北京航空航天大学",
    htmlContent: `
      <h2>1 前言</h2>
      <p>近年来，随着航空航天、传感器和信息技术的飞速发展，我国已进入高分辨率遥感卫星时代。高分辨率遥感影像目标检测在应急减灾、资源勘探、重大基础设施安全监测等领域的应用逐渐增多，尤其在应急减灾中发挥着重要作用。</p>
      <p>目标检测技术作为计算机视觉的核心任务，通过检测遥感图像中的目标，显著提升了灾后应急响应的效率与精度。因此，发展高分辨率遥感影像复杂目标检测技术已成为支撑灾害应急监测、预警与评估的重要手段。</p>
      <div class="image-container">
        <img src="/images/high-res-remote-1.png" alt="遥感影像智能解译在应急检测的应用" />
        <div class="image-caption">图1 遥感影像智能解译技术在应急检测的应用：覆盖地震、泥石流等重大自然灾害</div>
      </div>

      <h2>2 研究现状与技术挑战</h2>
      <p>高分辨率遥感影像不仅可以实时提供灾区的详细地理信息，还能够自动化地提取灾后场景的目标信息，为应急决策提供科学依据。在应急减灾中，通过遥感影像快速提取破坏区域的边界、受灾建筑物的数量及损毁程度，能够为救援部门提供高效的现场信息支持。</p>
      <p>尽管潜力巨大，实际应用中仍面临以下核心挑战：</p>
      <ul>
        <li>（1）复杂几何信息和密集目标的提取难度高：高分辨率遥感影像中，大视场目标的几何信息往往复杂且目标分布密集，现有技术难以处理目标遮挡、重叠及形态变异，导致几何特征难以精确提取。</li>
        <li>（2）尺度和视角变化带来的检测挑战：目标尺度差异大，且遥感平台高度、拍摄角度、光照条件变化会导致同一目标外观显著变化，现有方法难以保持一致检测效果。</li>
      </ul>

      <h2>3 高分辨率遥感影像复杂目标检测技术</h2>
      <p>该技术围绕“大视场目标几何信息表示难”的核心问题，通过三大创新方向实现高效、精准的目标感知：</p>
      <h3>3.1 多尺度遥感目标检测方法</h3>
      <p>针对大视场遥感图像“密集小目标检测难”的问题，探索遥感图像小目标感知空间和语义特征的纠缠耦合机理，提出<strong>动态感受野和细粒度任务解耦的多尺度检测方法</strong>，构建多尺度遥感目标检测感知框架，实现大视场密集小目标的精准感知。</p>

      <h3>3.2 双向几何轮廓表示技术</h3>
      <p>针对遥感图像中目标“几何结构不规则、自相交及非闭合”等难题，发明双向几何轮廓表示技术：</p>
      <ul>
        <li>引入矢量表示遥感目标的几何结构，突破传统像素级表示的局限性；</li>
        <li>建立基于“双向环”损失的序列约束，用于起始点与顺逆性无关的动态顶点间序列关系建模；</li>
        <li>实现高效的复杂形状目标检测和矢量化几何信息提取。</li>
      </ul>

      <h3>3.3 自适应几何结构表示框架</h3>
      <p>整合上述技术，构建复杂目标的自适应几何结构表示框架，实现多尺度、多类型遥感目标的灵活表示和精准感知，覆盖水体、道路、房屋等核心减灾目标。</p>
      <div class="image-container">
        <img src="/images/high-res-remote-2.png" alt="复杂目标检测技术框架" />
        <div class="image-caption">图2 高分辨率遥感影像复杂目标检测技术框架：解决特征耦合、感受野不匹配、目标形态复杂三大关键挑战</div>
      </div>

      <h3>3.4 减灾应用成果</h3>
      <p>该技术已成功应用于90余次全国重大自然灾害的灾后评估，包括：</p>
      <ul>
        <li>2021年舟曲立节滑坡</li>
        <li>2021年云南漾濞6.4级地震</li>
        <li>2021年河南郑州“7•20”特大暴雨灾害</li>
        <li>2022年四川泸定县6.8级地震</li>
        <li>2023年甘肃积石山6.2级地震</li>
      </ul>
      <p>核心成效：使灾后本底房屋分布制图由原来的1天提速至15分钟，大幅提升应急响应效率。该技术同时作为2024年度中国图象图形学学会技术发明奖的支持技术。</p>
      <div class="image-container">
        <img src="/images/high-res-remote-3.png" alt="水体提取结果" style="width:32%;margin-right:2%;" />
        <img src="/images/high-res-remote-4.png" alt="道路提取结果" style="width:32%;margin-right:2%;" />
        <img src="/images/high-res-remote-5.png" alt="房屋提取结果" style="width:32%;" />
        <div class="image-caption">图3-5 技术应用效果：左（水体提取）、中（道路提取）、右（房屋提取）</div>
      </div>

      <h2>4 发展趋势展望</h2>
      <p>随着遥感技术、深度学习和人工智能的不断进步，高分辨率遥感影像目标检测将在应急减灾领域发挥更为重要的作用，未来发展方向包括：</p>
      <ul>
        <li>（1）<strong>大模型驱动的多模态信息融合</strong>：将大模型应用于遥感影像的光谱、空间、时间等多维数据融合，学习不同模态间关联性，增强细节捕捉能力，动态调整融合策略以提高检测精度和可靠性。</li>
        <li>（2）<strong>基于生成式模型的几何结构表示</strong>：利用生成对抗网络（GAN）、扩散模型等模拟目标的多种几何形态，增强目标几何结构表示的鲁棒性，提升复杂场景下的表现。</li>
        <li>（3）<strong>人工智能与遥感领域的深度融合</strong>：结合自监督学习（利用大量未标注数据预训练）、强化学习等新兴技术，减少对标注数据的依赖，提高模型泛化能力和适应性。</li>
      </ul>
    `
  },
  // 第4篇：基于多模遥感图像和相关知识的建筑结构类型识别技术
  {
    title: "基于多模遥感图像和相关知识的建筑结构类型识别技术",
    subtitle: "华中科大研发 | 提升地震灾害抗震能力评估精度",
    meta: "周宇航、谭毅华、李郑兴 | 华中科技大学人工智能与自动化学院",
    htmlContent: `
      <h2>1 前言</h2>
      <p>建筑物是地震灾害的受灾主体，建筑物结构类型（如钢结构、钢筋混凝土结构、砖混结构）是评估其抗震能力的关键信息，对地震风险评估和应急响应至关重要。快速准确地获取建筑物结构类型信息，是防震减灾应用的核心需求之一。</p>
      <p>遥感技术具备多模态、高时空分辨率的特点，已被广泛用于提取建筑物结构类型相关特征；同时，建筑相关知识（如税务评估数据：占用率、建筑年龄、高度；地籍信息：楼层数、用途、兴趣点）与建筑物结构类型具有内在联系。充分利用这两种信息，是提高建筑物结构类型识别精度的重要手段。</p>

      <h2>2 研究现状与技术挑战</h2>
      <p>当前建筑物结构类型识别研究主要分为两类：</p>
      <ul>
        <li><strong>基于遥感图像的方法</strong>：利用单模或多模遥感数据提取光谱、空间和几何特征，结合传统机器学习或深度学习模型进行预测。但建筑物结构类型反映的是建筑内部结构，仅依靠遥感图像难以准确识别，限制了方法推广。</li>
        <li><strong>结合遥感图像与建筑相关知识的方法</strong>：引入税务评估数据、地籍信息等辅助信息提高识别精度，但通常采用人工构造特征或简单结合方式，特征挖掘能力有限，难以充分利用多源信息。</li>
      </ul>
      <p>现有研究仍面临三大技术挑战：</p>
      <ul>
        <li>（1）遥感数据的局限性：单一遥感数据难以反映建筑内部结构特征，多模遥感数据的高效融合仍是技术难题。</li>
        <li>（2）建筑相关知识的利用不足：现有方法未能充分利用知识图谱等技术建模建筑物结构类型相关信息，知识价值未被充分挖掘。</li>
        <li>（3）数据可获取性与方法通用性问题：依赖税务评估或地籍数据的方法在部分地区可能因数据缺失或过时而难以适用，需探索更广泛可获取的数据源。</li>
      </ul>

      <h2>3 基于多模遥感图像和相关知识的建筑结构类型识别</h2>
      <p>该系统是一种基于多模遥感影像和建筑相关知识的智能化识别系统，通过“三模块-三阶段”架构，在复杂场景下高效、精准完成建筑物结构类型的自动化识别任务。</p>

      <h3>3.1 系统核心架构</h3>
      <div class="image-container">
        <img src="/images/building-structure-1.png" alt="建筑结构类型识别框架图" />
        <div class="image-caption">图1 基于多模遥感图像和相关知识的建筑结构类型识别框架图</div>
      </div>
      <p>系统包含三个核心模块，分三阶段完成识别任务：</p>
      <h4>阶段1：特征提取</h4>
      <ul>
        <li><strong>图像特征提取模块</strong>：利用两个独立的卷积神经网络（CNN），分别从光学图像中提取建筑物外观、纹理特征，从合成孔径雷达（SAR）图像中提取材质特征（如钢结构与混凝土的雷达反射差异）。</li>
        <li><strong>语义特征提取模块</strong>：通过图注意网络（GAT）从建筑相关知识（税务数据、地籍信息）中挖掘语义信息，建立“建筑属性-结构类型”的关联关系（如高楼层建筑更可能为钢筋混凝土结构）。</li>
      </ul>

      <h4>阶段2：特征融合</h4>
      <ul>
        <li><strong>多模特征融合模块</strong>：通过智能筛选机制先融合光学图像与SAR图像的特征（去除冗余信息），再通过拼接方式整合语义特征，形成全面的融合特征，覆盖“外观-材质-属性”三维信息。</li>
      </ul>

      <h4>阶段3：类型识别</h4>
      <ul>
        <li>将融合后的特征输入全连接层，计算建筑物属于每种结构类型（钢结构、钢筋混凝土结构、砖混结构等）的概率，选择概率最高的类型作为最终结果。</li>
      </ul>

      <h3>3.2 数据收集框架</h3>
      <p>为解决数据来源单一导致的识别精度不足问题，设计多源数据收集框架：</p>
      <div class="image-container">
        <img src="/images/building-structure-2.png" alt="多源数据收集框架" />
        <div class="image-caption">图2 多源数据收集框架：整合光学影像、SAR影像、建筑相关知识</div>
      </div>
      <ul>
        <li>数据来源：不同平台的光学影像、合成孔径雷达影像、领域专家目视解译的建筑相关知识。</li>
        <li>数据精度保障：建筑物轮廓通过人工在专业平台上精确勾勒，建筑物结构类型通过地面调查获取（作为真实标签），确保数据准确性和可靠性。</li>
      </ul>

      <h3>3.3 识别效果验证</h3>
      <div class="image-container">
        <img src="/images/building-structure-3.png" alt="建筑物结构类型识别效果" />
        <div class="image-caption">图3 建筑物结构类型识别效果：左（光学图像）、中（真实标签）、右（预测结果），涵盖钢筋混凝土结构（reinforced concrete）、砖混结构（brick concrete）、钢结构（steel structure）</div>
      </div>
      <p>实验表明，该方法相比传统单模遥感方法，识别准确率提升15%-20%，尤其在复杂城区场景（多结构类型混合分布）中表现更优。</p>

      <h2>4 发展趋势展望</h2>
      <p>随着遥感大数据、多源异构数据融合及人工智能技术的快速发展，建筑物结构类型识别未来将围绕以下方向实现突破：</p>
      <ul>
        <li>（1）<strong>深化AI与遥感技术融合</strong>：利用深度学习大模型（遥感专用模型、视觉模型、语义推理模型）实现多源异构数据的全自动处理，结合领域知识图谱，更高效挖掘“影像特征-语义知识-结构类型”的深层关联，提升识别准确性和智能化水平。</li>
        <li>（2）<strong>强化多模态数据融合与语义建模</strong>：注重光学、SAR、LiDAR（激光雷达，获取三维结构）等多模态数据的深度融合，引入动态融合机制（根据场景自适应调整融合权重），增强模型在广域范围、复杂场景（如密集城区、山区建筑）的适应性和鲁棒性。</li>
      </ul>
    `
  },
  // 第5篇：基于双时相场景变化检测的弱监督灾后建筑物损毁遥感判识技术
  {
    title: "基于双时相场景变化检测的弱监督灾后建筑物损毁遥感判识技术",
    subtitle: "西南交大研发 | 解决灾后数据标注难题",
    meta: "慎利、乔文凡、郭梦晴 | 西南交通大学地球科学与工程学院",
    htmlContent: `
      <h2>1 前言</h2>
      <p>建筑密集区在自然灾害中往往是人员伤亡和财产损失最为严重的区域。因此，灾后迅速、准确地获取建筑物损毁信息，对于应急救援（如确定救援优先级）、决策指挥（如资源调配）以及灾后重建至关重要。</p>
      <p>遥感技术凭借其广泛的地表覆盖能力和快速响应特点，成为灾后评估的重要手段。随着高分辨率遥感影像的普及，大范围、高精度地提取灾后建筑物损毁成为可能。然而，传统人工解译方法效率低、耗时长，难以满足灾后紧急响应的时效性需求，亟需高效、自动化的建筑损毁判识方法。</p>

      <h2>2 研究现状与技术挑战</h2>
      <p>基于遥感的建筑损毁判识方法主要分为两类：</p>
      <ul>
        <li><strong>仅依赖灾后影像的方法</strong>：通过灾后影像中建筑物的破损特征（如屋顶坍塌、墙体断裂）进行判识，但受损建筑形态复杂、边界模糊，且易与周围环境干扰（如倒塌树木），判识效果有限。</li>
        <li><strong>结合灾前-灾后影像的双时相方法</strong>：通过对比灾前（完好状态）和灾后（可能损毁状态）影像的变化，提高损毁评估精度。但该方法仍面临三大核心挑战：</li>
      </ul>
      <ol>
        <li>建筑损毁的多样性与复杂性：受损建筑在形态、光谱属性和几何特征上差异大，且与周围环境相似性强，准确识别困难。</li>
        <li>影像变化检测中的干扰因素：易受非建筑物变化（如道路积水、树木倒伏）干扰，影响损毁区域定位精度。</li>
        <li>数据标注的挑战：像素级建筑损毁标注需大量人工干预，耗时且易出错；同时损毁建筑在影像中占比小，正负样本不平衡，增加模型训练难度。</li>
      </ol>

      <h2>3 基于场景变化检测的弱监督建筑物损毁遥感判识</h2>
      <p>该方法将“建筑物目标定位、场景变化检测、像素级损毁推断”集成到统一框架，通过弱监督学习减少对人工标注的依赖，核心分为三个步骤：</p>
      <div class="image-container">
        <img src="/images/building-damage-1.png" alt="建筑损毁判识总体设计框架" />
        <div class="image-caption">图1 基于场景变化检测的弱监督建筑物损毁遥感判识总体设计框架</div>
      </div>

      <h3>步骤1：子建筑对象生成</h3>
      <p>目的：过滤非建筑物干扰，生成精细化处理单元。</p>
      <ol>
        <li>通过语义分割网络（如DeepLab v3+）从灾前影像中提取建筑物区域，生成建筑区域掩膜，排除道路、绿地等非建筑区域。</li>
        <li>结合超像素分割（将影像分割为边界清晰的小区域）和区域邻接图，将建筑区域细化为“子建筑对象”（如一栋建筑分为多个子区域，避免因建筑过大导致的局部损毁漏判），保留建筑物边界细节，为后续精细化分析奠定基础。</li>
      </ol>

      <h3>步骤2：双时相子建筑场景变化检测</h3>
      <p>目的：精准检测子建筑对象在灾前-灾后的变化，判断是否损毁。</p>
      <div class="image-container">
        <img src="/images/building-damage-2.png" alt="局部全局视觉Transformer网络结构" />
        <div class="image-caption">图2 局部全局视觉Transformer网络结构（LgViTNet）：结合CNN局部特征与Transformer全局语义</div>
      </div>
      <p>基于子建筑对象，构建双时相孪生网络，核心模块包括：</p>
      <ul>
        <li><strong>局部-全局视觉Transformer结构（LgViTNet）</strong>：
          <ul>
            <li>结合卷积神经网络（CNN）的局部特征提取能力（捕捉屋顶破损、墙体裂缝等细节）与Transformer的全局语义建模能力（捕捉建筑整体结构变化）；</li>
            <li>通过逆残差块增强局部细节，利用自注意力机制捕捉跨区域依赖关系（如“屋顶坍塌”与“墙体断裂”的关联性）。</li>
          </ul>
        </li>
        <li><strong>跨孪生交互融合（CSIF）模块</strong>：
          <div class="image-container">
            <img src="/images/building-damage-3.png" alt="跨孪生交互融合模块" />
            <div class="image-caption">图3 跨孪生交互融合模块：动态筛选灾前-灾后特征差异</div>
          </div>
          <ul>
            <li>引入通道注意力机制，动态筛选灾前和灾后图像的特征差异，强化与损毁相关的关键区域（如坍塌部分）；</li>
            <li>通过交叉特征交互抑制非建筑变化（如道路积水、树木倒伏）的干扰，提高变化检测的针对性。</li>
          </ul>
        </li>
      </ul>
      <p>网络输入：以子建筑对象为中心裁剪的双时相图像块（256×256像素，包含灾前、灾后对应区域）；<br/>
         网络输出：“变化”（损毁）或“无变化”（完好）的标签。</p>

      <h3>步骤3：像素级建筑物损毁信息推断</h3>
      <p>目的：将场景级变化检测结果细化为像素级损毁地图，满足高精度评估需求。</p>
      <ul>
        <li>规则：若某个子建筑物被检测为“变化”（损毁），则其内部所有像素均标记为“损毁”；若为“无变化”，则标记为“完好”。</li>
        <li>优势：无需像素级标注，通过“子建筑对象-像素”的关联推断，解决样本标注成本高、正负样本不平衡的问题，同时确保损毁区域的完整性。</li>
      </ul>

      <h3>4 实验验证与应用效果</h3>
      <p>在海地太子港（地震灾区）和四川长宁（地震灾区）数据集上进行验证，结果如下：</p>
      <div class="image-container">
        <img src="/images/building-damage-4.png" alt="海地太子港地区损毁判识结果" style="width:48%;margin-right:2%;" />
        <img src="/images/building-damage-5.png" alt="四川长宁地区损毁判识结果" style="width:48%;" />
        <div class="image-caption">图4-5 建筑物损毁判识结果：左（海地太子港）、右（四川长宁），从左至右依次为灾前影像、灾后影像、真实标签、检测结果</div>
      </div>
      <p><strong>核心指标</strong>：在两个数据集上，损毁判识的F1分数均超过0.85，显著优于传统双时相变化检测方法（F1约0.75），且标注成本降低90%以上（无需像素级标注，仅需少量子建筑对象标签）。</p>

      <h2>5 发展趋势展望</h2>
      <p>未来建筑物损毁遥感判识技术将围绕以下方向突破，进一步提升应急响应能力：</p>
      <ul>
        <li>（1）<strong>多模态数据与知识协同驱动</strong>：融合光学影像、SAR影像、LiDAR点云（获取三维损毁信息），结合建筑结构类型、抗震等级、历史灾害数据等领域知识图谱，提高模型物理可解释性和预测精度。</li>
        <li>（2）<strong>生成式模型与多视角观测</strong>：利用生成对抗网络（GAN）、扩散模型模拟建筑损毁的多样化形态（如坍塌、倾斜），生成高保真合成数据解决样本稀缺问题；结合无人机倾斜摄影与正射影像的多视角分析，提升损毁程度（如轻度破损、完全坍塌）评估的精确度。</li>
        <li>（3）<strong>大模型驱动的自适应学习</strong>：基于遥感专用大模型，通过自监督预训练和少量标注数据微调，解决灾后数据稀缺区域的泛化问题；采用无监督域自适应策略，减少对标注数据的依赖，实现跨灾种（地震、洪水、台风）的快速适配。</li>
        <li>（4）<strong>边缘计算与实时响应优化</strong>：通过神经网络架构搜索和模型量化技术优化计算量，适配无人机和边缘计算设备，实现灾后损毁的实时监测；结合无人机集群与边缘服务器，建立“感知-推理-决策”闭环，提高灾后响应效率与决策精准度。</li>
      </ul>
    `
  }
]);

// 获取路由参数（如/technology/1中的"1"）
const route = useRoute();
const docId = computed(() => {
  // 将路由参数id转换为数字，默认取1（防止参数错误）
  return parseInt(route.params.id) || 1;
});

// 根据路由参数匹配当前文章（docList索引从0开始，故减1）
const currentDoc = computed(() => {
  const index = docId.value - 1;
  // 若索引超出范围（如id=6），默认返回第一篇文章
  return docList.value[index] || docList.value[0];
});
</script>

<style scoped>

.page-container {
  background-color: #f5f7fa;
  min-height: 100vh;
  padding: 20px;
}


.image-container {
  margin: 20px 0;
  text-align: center;
}

.image-container img {
  max-width: 100%;
  height: auto;
  border: 1px solid #eee;
  border-radius: 4px;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.image-caption {
  margin-top: 8px;
  font-size: 14px;
  color: #666;
  font-style: italic;
}


table {
  border-collapse: collapse;
  width: 100%;
  margin: 16px 0;
}

table th, table td {
  border: 1px solid #ddd;
  padding: 8px;
  text-align: left;
}

table th {
  background-color: #f5f5f5;
  font-weight: bold;
}


h2 {
  margin: 24px 0 16px;
  font-size: 20px;
  color: #2c3e50;
  border-bottom: 1px solid #eee;
  padding-bottom: 8px;
}

h3 {
  margin: 20px 0 12px;
  font-size: 18px;
  color: #34495e;
}

h4 {
  margin: 16px 0 8px;
  font-size: 16px;
  color: #4a6fa5;
}

p {
  margin: 12px 0;
  line-height: 1.6;
  color: #333;
}


ul, ol {
  margin: 12px 0 12px 24px;
  line-height: 1.6;
}

ul li, ol li {
  margin: 8px 0;
}
</style>